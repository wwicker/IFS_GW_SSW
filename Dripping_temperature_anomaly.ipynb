{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53003694-470e-44fe-848e-1951dcc1c3d5",
   "metadata": {},
   "source": [
    "# Dripping temperature anomaly\n",
    "\n",
    "Create Figure 3, A1, A2, and B2. This requires temperature and meridional velocity for the simulations:\n",
    "\n",
    "- TCo639_free_running/20060117_91L (hopg)\n",
    "- TCo639_free_running/20060117_137L (hpp0)\n",
    "- TCo639_free_running/20060117_198L (hopf)\n",
    "- TCo639_free_running/20100205_91L (hopd)\n",
    "- TCo639_free_running/20100205_137L (hpp1)\n",
    "- TCo639_free_running/20100205_198L (hope)\n",
    "- TCo639_free_running/20180208_91L (hokw)\n",
    "- TCo639_free_running/20180208_137L (hpoz)\n",
    "- TCo639_free_running/20180208_198L (hokx)\n",
    "\n",
    "It also requires processed reanalysis data for climatologies or polar cap temperature anomalies and planetary wave meridional heat flux, as well as timeseries for the SSW events in 2006, 2010, and 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756b48bb-2fc5-449a-9825-05962edf6ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "import scipy.stats as stats\n",
    "import scipy.ndimage\n",
    "import numba\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.transforms as mtransforms\n",
    "import cmocean\n",
    "import os\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f034cb-4398-47e6-bcf6-1f126a4b2394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_Grib(directory,chunks={}):\n",
    "    '''\n",
    "    '''\n",
    "    files = [directory+f for f in os.listdir(directory) if f.endswith('.grb')]\n",
    "    files.sort()\n",
    "    ds = xr.open_mfdataset(files,engine='cfgrib',chunks=chunks,combine='nested',concat_dim='number')\n",
    "    \n",
    "    return ds\n",
    "\n",
    "\n",
    "def resample2daily(ds):\n",
    "    '''\n",
    "        Pandas' default behaviour for resampling is to put the time stamp at the beginning of the bin.\n",
    "    '''\n",
    "    # Rename dimensions\n",
    "    try:\n",
    "        ds = ds.drop(('step','time')).set_index(step='valid_time').rename(step='time')\n",
    "    except:\n",
    "        print('Exception: Time dimension is not renamed')\n",
    "    if 'isobaricInhPa' in ds.dims:\n",
    "        ds = ds.rename(isobaricInhPa='level')\n",
    "        \n",
    "    attrs = {}\n",
    "    for var in ds.data_vars:\n",
    "        try:\n",
    "            attrs.update({var:dict(standard_name=ds[var].attrs['standard_name'],units=ds[var].attrs['units'])})\n",
    "        except KeyError:\n",
    "            try:\n",
    "                attrs.update({var:dict(standard_name=ds[var].attrs['long_name'],units=ds[var].attrs['units'])})\n",
    "            except:\n",
    "                print('Exception: No name attribute') \n",
    "    \n",
    "    # Resample to daily mean\n",
    "    ds = ds.resample(time='1D').mean()\n",
    "\n",
    "    for var in ds.data_vars:\n",
    "        try:\n",
    "            ds[var].attrs = attrs[var]\n",
    "        except:\n",
    "            # No name attribute\n",
    "            print('')\n",
    "        \n",
    "    return ds\n",
    "\n",
    "\n",
    "def area_weighted_mean(da,dim=('latitude','longitude')):\n",
    "    '''\n",
    "        Weight data on regular lon-lat grid with cosine of latitude\n",
    "    '''\n",
    "    weights = np.cos(da['latitude'] * np.pi/180)\n",
    "    da = da * weights\n",
    "    da = da.mean(dim=dim)\n",
    "    da = da / weights.mean(dim='latitude')\n",
    "    return da\n",
    "    \n",
    "\n",
    "def t_polar_anomal(sample,area = {'latitude':slice(90,60)}):\n",
    "    '''\n",
    "        Polar cap tempearture anomaly\n",
    "    '''\n",
    "    t = sample['t'].sel(**area)\n",
    "    t = area_weighted_mean(t)\n",
    "    t = t.compute()\n",
    "    clim = xr.open_dataset(work_dir+'reanalysis_t_climatology.nc')\n",
    "    clim = clim.sel(**area)\n",
    "    clim = area_weighted_mean(clim)\n",
    "    t = t.groupby('time.dayofyear') - clim['t']\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7767eebd-9297-4d53-9203-852e818f8be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavenumber_decomposition(da,wavenum):\n",
    "    '''\n",
    "    '''\n",
    "    lon = da.longitude\n",
    "    k = xr.DataArray(np.fft.rfftfreq(len(lon),d=1/len(lon)),dims=('wavenumber'))\n",
    "    # function that accepts dummy argument\n",
    "    rfft = lambda da,k: dask.array.fft.rfft(da)\n",
    "    irfft = lambda da,lon: dask.array.fft.irfft(da)\n",
    "    \n",
    "    transform = xr.apply_ufunc(rfft,\n",
    "                               da,k,\n",
    "                               input_core_dims=[['longitude'],['wavenumber']],\n",
    "                               output_core_dims=[['wavenumber'],],\n",
    "                               dask='allowed',\n",
    "                               output_dtypes=[np.complex_]\n",
    "                              )\n",
    "    transform['wavenumber'] = k\n",
    "    \n",
    "    decomposed = []\n",
    "    for k in wavenum:\n",
    "        tmp = transform.where(transform.wavenumber==k,other=0)\n",
    "        tmp = xr.apply_ufunc(irfft,\n",
    "                             tmp,lon,\n",
    "                             input_core_dims=[['wavenumber'],['longitude']],\n",
    "                             output_core_dims=[['longitude'],],\n",
    "                             dask='allowed',\n",
    "                             output_dtypes=[np.float_]\n",
    "                            )\n",
    "        decomposed.append(tmp.assign_coords(wavenumber=k))\n",
    "        \n",
    "    decomposed = xr.concat(decomposed,dim='wavenumber')\n",
    "    decomposed = decomposed.sum('wavenumber')\n",
    "    \n",
    "    return decomposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aa67c4-0fc9-45ed-9f86-78438c35e9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_sample_t_test(x0,sample,dim='number',broadcast_dims=('time','level')):\n",
    "    '''\n",
    "        Comparing sample mean against true value\n",
    "        \n",
    "        H0: \\mu = x0\n",
    "        \n",
    "        Returns the probability that x1 - x0 larger than the observed value could have occurded by chance\n",
    "    '''\n",
    "    n = len(sample[dim])\n",
    "    x1 = sample.mean(dim)\n",
    "    s1 = sample.var(dim)\n",
    "    \n",
    "    t = (x1-x0) / np.sqrt(s1/n)\n",
    "    dof = n-1\n",
    "    \n",
    "    p = xr.apply_ufunc(stats.t.sf,t,dof,\n",
    "                       input_core_dims=[broadcast_dims,broadcast_dims],\n",
    "                       output_core_dims=[broadcast_dims],\n",
    "                       output_dtypes=t.dtype)\n",
    "    return p\n",
    "\n",
    "\n",
    "def two_sample_t_test(sample1,sample2,dim='number',broadcast_dims=('time','level')):\n",
    "    '''\n",
    "        Comparing two samples means, independent samples\n",
    "        \n",
    "        Be \\sigma_1^2 \\ne \\sigma_2^2, but n_1=n_2=n\n",
    "        H0: \\mu_1 = \\mu_2\n",
    "        \n",
    "        Returns the probability that x1 - x2 larger than the observed value could have occurded by chance\n",
    "    '''\n",
    "    n = len(sample1[dim])\n",
    "    x1 = sample1.mean(dim)\n",
    "    s1 = sample1.var(dim)\n",
    "    x2 = sample2.mean(dim)\n",
    "    s2 = sample2.var(dim)\n",
    "    \n",
    "    t = (x1-x2) / np.sqrt((s1+s2)/n)\n",
    "    dof = n - 1 + (2 * n - 2) / (s1 / s2 + s2 / s1)\n",
    "\n",
    "    p = xr.apply_ufunc(stats.t.sf,t,dof,\n",
    "                       input_core_dims=[broadcast_dims,broadcast_dims],\n",
    "                       output_core_dims=[broadcast_dims],\n",
    "                       output_dtypes=t.dtype)\n",
    "    return p\n",
    "\n",
    "\n",
    "@numba.guvectorize(\n",
    "    \"(float64[:], float64[:], float64[:])\",\n",
    "    \"(m), (n) -> (m)\",\n",
    "    forceobj=True\n",
    ")\n",
    "def vectorized_convolution(x,kernel,out):\n",
    "    '''\n",
    "        Vectorized convolution -> generalized NumPy universal function\n",
    "        \n",
    "        - mode='wrap' means that input is assumed being periodic\n",
    "    '''\n",
    "    out[:] = scipy.ndimage.convolve(x,kernel,mode='wrap')\n",
    "\n",
    "\n",
    "def lowpass(da,dim,co,valid=False):\n",
    "    '''\n",
    "        convolution in time space is multiplication in frequency space\n",
    "        \n",
    "        - no chunking along core dimension dim\n",
    "    '''\n",
    "    # transform of Hanning window as better spectral properties than transfrom of box-car-window\n",
    "    n = 2 * co  + 1\n",
    "    hann = np.hanning(n)\n",
    "    hann /= hann.sum()\n",
    "    hann = xr.DataArray(hann,dims=('kernel'))\n",
    "    \n",
    "    filtered = xr.apply_ufunc(vectorized_convolution,\n",
    "                              da,hann,                                           \n",
    "                              input_core_dims=[[dim,],['kernel']],\n",
    "                              output_core_dims=[[dim,],],\n",
    "                              dask='parallelized',\n",
    "                              output_dtypes=[da.dtype])\n",
    "    \n",
    "    # input is assumed to be periodic\n",
    "    # remove beginning and end if unvalid\n",
    "    if valid:\n",
    "        valid_slice = slice((n - 1) // 2, -(n - 1) // 2)\n",
    "        filtered = filtered.isel(time=valid_slice)\n",
    "        \n",
    "    return filtered\n",
    "\n",
    "\n",
    "def climatology(da,ref):\n",
    "    clim = da.groupby('time.dayofyear').mean('time').compute()\n",
    "    clim = lowpass(clim,'dayofyear',30)\n",
    "    clim = clim.where(clim['dayofyear'].isin(ref['time.dayofyear'].values),drop=True)\n",
    "    clim = clim.drop('dayofyear').rename(dayofyear='time').assign_coords(time=ref.time)\n",
    "    return clim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a785f68-895d-4877-9392-60e33abdaa3a",
   "metadata": {},
   "source": [
    "## Processing data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cdb56854-ffe3-43ef-af86-157a2155bbb0",
   "metadata": {},
   "source": [
    "# climatology from reanalyis to calculate temperature anomalies\n",
    "t_files = [reanalysis_dir+f for f in os.listdir(reanalysis_dir) if f.endswith('.nc')]\n",
    "t_files.sort()\n",
    "\n",
    "T = xr.open_mfdataset(t_files,chunks={},combine='nested',concat_dim='time')\n",
    "clim = T['t'].groupby('time.dayofyear').mean().compute()\n",
    "clim = lowpass(clim,'dayofyear',30)\n",
    "xr.Dataset({'t':clim}).to_netcdf(data_dir+'reanalysis/reanalysis_t_climatology.nc')\n",
    "\n",
    "clim"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee44349e-9305-4608-a15f-742b2257ac07",
   "metadata": {},
   "source": [
    "#startdate = '20180129'\n",
    "#timeslice = ['2018-01-29T00:00','2018-03-25T18:00']\n",
    "\n",
    "#startdate = '20100122'\n",
    "#timeslice = ['2010-01-22T00:00','2010-03-21T18:00']\n",
    "\n",
    "startdate = '20060103'\n",
    "timeslice = ['2006-01-03T00:00','2006-03-02T18:00']\n",
    "\n",
    "files = files = [reanalysis_dir+f for f in os.listdir(reanalysis_dir) if f.endswith('.nc')]\n",
    "data = xr.open_mfdataset(files,combine='nested',concat_dim='time')\n",
    "data = data.sortby('time')\n",
    "\n",
    "sample = data.sel(time=slice(np.datetime64(timeslice[0]),np.datetime64(timeslice[1])))\n",
    "\n",
    "print(sample)\n",
    "\n",
    "paint = t_polar_anomal(sample)\n",
    "filename = startdate+'_era5_paint_t_polar_anomal.nc'\n",
    "print(filename)\n",
    "xr.Dataset(dict(t=paint)).to_netcdf(data_dir+'reanalysis/'+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4242ad89-6f6b-4e78-be22-820355b5fa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store polar cap temperature anomalies to zwischenspeicher\n",
    "# model configurations\n",
    "\n",
    "directory = lambda date, config: data_dir+date+'_'+config+'/pressure_levels_F64/t/'\n",
    "\n",
    "startdates = ['TCo639_free_running/20180208',\n",
    "              'TCo639_free_running/20060117',\n",
    "              'TCo639_free_running/20100205'\n",
    "              ]\n",
    "configurations = ['91L','198L','137L',]\n",
    "\n",
    "for date in startdates:\n",
    "    for config in configurations:\n",
    "        \n",
    "        sample = load_Grib(directory(date,config))\n",
    "        sample = resample2daily(sample)\n",
    "        paint = t_polar_anomal(sample)\n",
    "        \n",
    "        filename = date+'_'+config+'_paint_t_polar_anomal.nc'\n",
    "        print(filename)\n",
    "        xr.Dataset(dict(t=paint)).to_netcdf(work_dir+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc3b7dc-c135-4ead-9d1b-d52bd932f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = ['TCo639_free_running/20060117_91L',\n",
    "               'TCo639_free_running/20060117_198L',\n",
    "               'TCo639_free_running/20100205_198L',\n",
    "               'TCo639_free_running/20100205_91L',\n",
    "               'TCo639_free_running/20180208_91L',\n",
    "               'TCo639_free_running/20180208_198L',\n",
    "               ]\n",
    "\n",
    "for exp in experiments:\n",
    "    \n",
    "    print(exp)\n",
    "    \n",
    "    v = load_Grib(data_dir+exp+'/pressure_levels_F64/v/',chunks=dict(step=16))\n",
    "    v = resample2daily(v)['v']\n",
    "    v = wavenumber_decomposition(v,[1,2,3]).persist()\n",
    "    \n",
    "    t = load_Grib(data_dir+exp+'/pressure_levels_F64/t/',chunks=dict(step=16))\n",
    "    t = resample2daily(t)['t']\n",
    "    t = wavenumber_decomposition(t,[1,2,3]).persist()\n",
    "    flux = v * t\n",
    "    flux = flux.mean('longitude').compute()\n",
    "    \n",
    "    xr.Dataset(dict(flux=flux)).to_netcdf(work_dir+exp+'_planetary_heat_flux.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e917c2-ae8e-442f-85a6-4b83c6be5a38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "881192f0-498a-4442-baae-a03759760868",
   "metadata": {},
   "source": [
    "## Figure 3\n",
    "\n",
    "Polar-cap temperature anomalies averaged between 60 and 90° N during the 2018 SSW event. The central date of the event is marked by the reversal of zonal-mean westerlies at 10 hPa and 60° N indicated by the vertical line. (a) The development of temperature anomalies [K] in the reanalysis, (b) the ensemble-mean bias of TCo639L91 hindcasts initialized on 8 February compared to the reanalysis, and (c) the improvement in the TCo639L198 hindcast\n",
    "ensemble mean compared to the TCo639L91 hindcasts. Hatching indicates areas where ensemble-mean differences are not significantly different from zero at a 95% confidence level estimated by a one-sample (b) or two-sample t test (c). Panel (d) shows the ensemble-mean meridional eddy heat flux by zonal wavenumbers 1 to 3 at 100 hPa averaged between 45 and 70° N as a measure of the planetary wave flux in the lower stratosphere for climatology (dashed blue), the event in the reanalysis (orange), and the two model configurations (green and red)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebe3a77-eaf8-4cbc-923c-4d6d89e2d3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dripping_temperature(reanalysis,L91,L198,\n",
    "                         reanalysis_flux,L91_flux,L198_flux,\n",
    "                         central_date):\n",
    "    '''\n",
    "    '''\n",
    "    # load temperature data and compute plotables\n",
    "    ref = xr.open_dataarray(reanalysis)\n",
    "    ref = ref.sortby('level',ascending=False)\n",
    "    ref['time'] = ref['time'] - np.timedelta64(9,'h')\n",
    "    \n",
    "    paint1 = xr.open_dataarray(L91)\n",
    "    \n",
    "    bias = paint1.mean('number') - ref.interp_like(paint1,method='nearest')\n",
    "    p = one_sample_t_test(ref, paint1)\n",
    "    bias_sig = np.add(p < 0.025, p > 0.975)\n",
    "    \n",
    "    paint2 = xr.open_dataarray(L198)\n",
    "    \n",
    "    improvement = paint2.mean('number') - paint1.mean('number')\n",
    "    p = two_sample_t_test(paint1, paint2)\n",
    "    improvement_sig = np.add(p < 0.025, p > 0.975)\n",
    "    \n",
    "    # load temperature flux\n",
    "    L91 = xr.open_dataarray(L91_flux)\n",
    "    L91 = area_weighted_mean(L91.sel(latitude=slice(70,45),level=100),dim='latitude').mean('number').drop('level')\n",
    "    L198 = xr.open_dataarray(L198_flux)\n",
    "    L198 = area_weighted_mean(L198.sel(latitude=slice(70,45),level=100),dim='latitude').mean('number').drop('level')\n",
    "    reanalysis = xr.open_dataarray(reanalysis_flux)\n",
    "    reanalysis = reanalysis.interp(time=ref.time)\n",
    "    reanalysis = area_weighted_mean(reanalysis.sel(latitude=slice(70,45),level=100),dim='latitude').drop('level')    \n",
    "    \n",
    "    \n",
    "    clim = xr.open_dataarray(data_dir+'reanalysis/reanalysis_planetary_heat_flux.nc')\n",
    "    clim = area_weighted_mean(climatology(clim.sel(latitude=slice(70,45),level=100),ref),dim='latitude').drop('level')\n",
    "\n",
    "    \n",
    "    # Plotting\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=4,sharex='all',figsize=(6,9))\n",
    "    \n",
    "    C1 = ref.plot.pcolormesh(ax=axes[0],x='time',cmap=cmocean.cm.balance,extend='both',\n",
    "                        levels=np.linspace(-25,25,33),add_colorbar=False)\n",
    "    axes[0].plot([np.datetime64(central_date),np.datetime64(central_date)],[0,1000],'k-')\n",
    "    \n",
    "    ax = axes[0].twinx()\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('reanalysis',labelpad=80,size=18,weight='bold')\n",
    "    ax.yaxis.set_label_position('left')\n",
    "    \n",
    "    xlim = axes[0].get_xlim()\n",
    "    \n",
    "    \n",
    "    C2 = bias.plot.pcolormesh(ax=axes[1],x='time',cmap=cmocean.cm.balance,extend='both',\n",
    "                        levels=np.linspace(-6,6,33),add_colorbar=False)\n",
    "    bias_sig.astype(np.double).plot.contourf(ax=axes[1],x='time',levels=[0,0.5,1],hatches=['\\\\',''],\n",
    "                                             alpha=0,add_colorbar=False)\n",
    "    \n",
    "    ax = axes[1].twinx()\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('L91 bias',labelpad=80,size=18,weight='bold')\n",
    "    ax.yaxis.set_label_position('left')\n",
    "    \n",
    "    \n",
    "    improvement.plot.pcolormesh(ax=axes[2],x='time',cmap=cmocean.cm.balance,extend='both',\n",
    "                               levels=np.linspace(-6,6,33),add_colorbar=False)\n",
    "    improvement_sig.astype(np.double).plot.contourf(ax=axes[2],x='time',levels=[0,0.5,1],hatches=['\\\\',''],\n",
    "                                                   alpha=0,add_colorbar=False)\n",
    "    \n",
    "    ax = axes[2].twinx()\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('L198 - L91',labelpad=80,size=18,weight='bold')\n",
    "    ax.yaxis.set_label_position('left')\n",
    "    \n",
    "    clim.plot.line(ax=axes[3],label='climatology',linestyle='dotted')\n",
    "    reanalysis.plot.line(ax=axes[3],label='reanalysis',linestyle='solid')\n",
    "    L91.plot.line(ax=axes[3],label='TCo639L91',linestyle='dashed')\n",
    "    L198.plot.line(ax=axes[3],label='TCo639L198',linestyle='dashdot')\n",
    "    axes[3].legend()\n",
    "    \n",
    "    ax = axes[3].twinx()\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('heat flux',labelpad=80,size=18,weight='bold')\n",
    "    ax.yaxis.set_label_position('left')\n",
    "    \n",
    "    \n",
    "    for ax in axes[:3]:\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_ylim(ax.get_ylim()[::-1])\n",
    "        ax.set_ylabel('pressure [hPa]')\n",
    "        ax.set_xlabel(None)\n",
    "        ax.set_xlim(xlim)\n",
    "    \n",
    "    axes[3].set_ylabel(r'[K m s$^{-1}$]')\n",
    "    axes[3].set_xlim(xlim)\n",
    "    axes[3].set_xlabel('time')\n",
    "    \n",
    "    trans = mtransforms.ScaledTranslation(-45/72, -20/72, fig.dpi_scale_trans)\n",
    "    \n",
    "    axes[0].text(-0.06,1.0,'a)',transform=axes[0].transAxes+trans,fontsize='large',va='bottom',fontfamily='serif')\n",
    "    axes[1].text(-0.06,1.0,'b)',transform=axes[1].transAxes+trans,fontsize='large',va='bottom',fontfamily='serif')\n",
    "    axes[2].text(-0.06,1.0,'c)',transform=axes[2].transAxes+trans,fontsize='large',va='bottom',fontfamily='serif')\n",
    "    axes[3].text(-0.06,1.0,'d)',transform=axes[3].transAxes+trans,fontsize='large',va='bottom',fontfamily='serif')\n",
    "    \n",
    "    fig.subplots_adjust(0,0,1,0.95,0,0)\n",
    "    \n",
    "    cbar = plt.colorbar(C1,ax=axes[0],orientation='vertical',fraction=0.1,aspect=10,shrink=0.95)\n",
    "    cbar.set_label(r'[K]')\n",
    "    \n",
    "    cbar = plt.colorbar(C2,ax=axes[1:3],orientation='vertical',fraction=0.1,shrink=0.95)\n",
    "    cbar.set_label(r'[K]')\n",
    "    \n",
    "    box = axes[3].get_position().get_points()\n",
    "    box[1,0] = axes[2].get_position().get_points()[1,0]\n",
    "    axes[3].set_position(box.flatten())\n",
    "    \n",
    "    \n",
    "dripping_temperature(work_dir+'20180129_era5_paint_t_polar_anomal.nc',\n",
    "                     work_dir+'TCo639_free_running/20180208_91L_paint_t_polar_anomal.nc',\n",
    "                     work_dir+'TCo639_free_running/20180208_198L_paint_t_polar_anomal.nc',\n",
    "                     work_dir+'era5_2018_planetary_heat_flux.nc',\n",
    "                     work_dir+'TCo639_free_running/20180208_91L_planetary_heat_flux.nc',\n",
    "                     work_dir+'TCo639_free_running/20180208_198L_planetary_heat_flux.nc',\n",
    "                     '2018-02-12',)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c592a4f9-748e-41e4-a368-afd11c5577d3",
   "metadata": {},
   "source": [
    "## Figure A1\n",
    "\n",
    "Same as Fig. 3 but for the SSW event in 2006."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4d167e-c94a-4906-912b-b37f09af4d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "dripping_temperature(work_dir+'20060103_era5_paint_t_polar_anomal.nc',\n",
    "                     work_dir+'TCo639_free_running/20060117_91L_paint_t_polar_anomal.nc',\n",
    "                     work_dir+'TCo639_free_running/20060117_198L_paint_t_polar_anomal.nc',\n",
    "                     work_dir+'era5_2006_planetary_heat_flux.nc',\n",
    "                     work_dir+'TCo639_free_running/20060117_91L_planetary_heat_flux.nc',\n",
    "                     work_dir+'TCo639_free_running/20060117_198L_planetary_heat_flux.nc',\n",
    "                     '2006-01-21',)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c805889a-3a22-4a41-9925-ed42c0029c46",
   "metadata": {},
   "source": [
    "## Figure A2\n",
    "\n",
    "Same as Fig. 3 but for the SSW event in 2010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7644848f-68f1-463f-8e7d-5e01544d650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dripping_temperature(work_dir+'20100122_era5_paint_t_polar_anomal.nc',\n",
    "                     work_dir+'TCo639_free_running/20100205_91L_paint_t_polar_anomal.nc',\n",
    "                     work_dir+'TCo639_free_running/20100205_198L_paint_t_polar_anomal.nc',\n",
    "                     work_dir+'era5_2010_planetary_heat_flux.nc',\n",
    "                     work_dir+'TCo639_free_running/20100205_91L_planetary_heat_flux.nc',\n",
    "                     work_dir+'TCo639_free_running/20100205_198L_planetary_heat_flux.nc',\n",
    "                     '2010-02-09',)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef717d2e-0b48-419f-ba94-3131304ce1ff",
   "metadata": {},
   "source": [
    "## Figure B2\n",
    "\n",
    "Differences in the TCo639L198 hindcast ensemblemean polar-cap temperature anomalies during the 2006, the 2010, and the 2018 SSW events compared to the TCo639L137 hindcasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645f3588-9de6-4416-9063-05eac360897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_three_differences(L137_1,L198_1,\n",
    "                           L137_2,L198_2,\n",
    "                           L137_3,L198_3,):\n",
    "    \n",
    "    # load temperature data and compute plotables for first startdate\n",
    "    paint1 = xr.open_dataarray(L137_1)\n",
    "    paint2 = xr.open_dataarray(L198_1)\n",
    "    \n",
    "    difference_1 = paint2.mean('number') - paint1.mean('number')\n",
    "    p = two_sample_t_test(paint1, paint2)\n",
    "    sig_1 = np.add(p < 0.025, p > 0.975)\n",
    "    \n",
    "    # load temperature data and compute plotables for second startdate\n",
    "    paint1 = xr.open_dataarray(L137_2)\n",
    "    paint2 = xr.open_dataarray(L198_2)\n",
    "    \n",
    "    difference_2 = paint2.mean('number') - paint1.mean('number')\n",
    "    p = two_sample_t_test(paint1, paint2)\n",
    "    sig_2 = np.add(p < 0.025, p > 0.975)\n",
    "    \n",
    "    # load temperature data and compute plotables for third startdate\n",
    "    paint1 = xr.open_dataarray(L137_3)\n",
    "    paint2 = xr.open_dataarray(L198_3)\n",
    "    \n",
    "    difference_3 = paint2.mean('number') - paint1.mean('number')\n",
    "    p = two_sample_t_test(paint1, paint2)\n",
    "    sig_3 = np.add(p < 0.025, p > 0.975)\n",
    "    \n",
    "    # plotting\n",
    "    fig, axes = plt.subplots(nrows=3,ncols=1,sharex=False,sharey=True,figsize=(8,12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    C = difference_1.plot.pcolormesh(ax=axes[0],x='time',cmap=cmocean.cm.balance,extend='both',\n",
    "                        levels=np.linspace(-6,6,33),add_colorbar=False)\n",
    "    sig_1.astype(np.double).plot.contourf(ax=axes[0],x='time',levels=[0,0.5,1],hatches=['\\\\',''],\n",
    "                                          alpha=0,add_colorbar=False)\n",
    "    \n",
    "    difference_2.plot.pcolormesh(ax=axes[1],x='time',cmap=cmocean.cm.balance,extend='both',\n",
    "                        levels=np.linspace(-6,6,33),add_colorbar=False)\n",
    "    sig_2.astype(np.double).plot.contourf(ax=axes[1],x='time',levels=[0,0.5,1],hatches=['\\\\',''],\n",
    "                                          alpha=0,add_colorbar=False)\n",
    "    \n",
    "    difference_3.plot.pcolormesh(ax=axes[2],x='time',cmap=cmocean.cm.balance,extend='both',\n",
    "                        levels=np.linspace(-6,6,33),add_colorbar=False)\n",
    "    sig_3.astype(np.double).plot.contourf(ax=axes[2],x='time',levels=[0,0.5,1],hatches=['\\\\',''],\n",
    "                                          alpha=0,add_colorbar=False)\n",
    "    \n",
    "    for ax in axes:\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_ylim(ax.get_ylim()[::-1])\n",
    "        ax.set_ylabel('pressure [hPa]')\n",
    "        ax.set_xlabel(None)\n",
    "    axes[2].set_xlabel('time')\n",
    "        \n",
    "    trans = mtransforms.ScaledTranslation(-45/72, -20/72, fig.dpi_scale_trans)\n",
    "    \n",
    "    axes[0].text(-0.06,1.0,'a)',transform=axes[0].transAxes+trans,fontsize='large',va='bottom',fontfamily='serif')\n",
    "    axes[1].text(-0.06,1.0,'b)',transform=axes[1].transAxes+trans,fontsize='large',va='bottom',fontfamily='serif')\n",
    "    axes[2].text(-0.06,1.0,'c)',transform=axes[2].transAxes+trans,fontsize='large',va='bottom',fontfamily='serif')\n",
    "    \n",
    "    fig.subplots_adjust(0,0,1,1,0,0.4)\n",
    "    \n",
    "    ax = axes[0].twinx()\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('2006',labelpad=80,size=18,weight='bold')\n",
    "    ax.yaxis.set_label_position('left')\n",
    "    \n",
    "    ax = axes[1].twinx()\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('2010',labelpad=80,size=18,weight='bold')\n",
    "    ax.yaxis.set_label_position('left')\n",
    "    \n",
    "    ax = axes[2].twinx()\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('2018',labelpad=80,size=18,weight='bold')\n",
    "    ax.yaxis.set_label_position('left')\n",
    "    \n",
    "    cbar = plt.colorbar(C,ax=axes,orientation='horizontal',fraction=0.05,pad=0.1)\n",
    "    cbar.set_label(r'L198 - L137 [K]')\n",
    "    \n",
    "    \n",
    "plot_three_differences(work_dir+'TCo639_free_running/20060117_137L_paint_t_polar_anomal.nc',\n",
    "                       work_dir+'TCo639_free_running/20060117_198L_paint_t_polar_anomal.nc',\n",
    "                       work_dir+'TCo639_free_running/20100205_137L_paint_t_polar_anomal.nc',\n",
    "                       work_dir+'TCo639_free_running/20100205_198L_paint_t_polar_anomal.nc',\n",
    "                       work_dir+'TCo639_free_running/20180208_137L_paint_t_polar_anomal.nc',\n",
    "                       work_dir+'TCo639_free_running/20180208_198L_paint_t_polar_anomal.nc',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dcd6e8-419d-48c0-8886-8c18ad428639",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:IFS_GW_SSW]",
   "language": "python",
   "name": "conda-env-IFS_GW_SSW-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
